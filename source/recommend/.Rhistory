data("USArrests")      # Loading the data set
df <- scale(USArrests) # Scaling the data
# View the firt 3 rows of the data
head(df, n = 3)
View(df)
print(km.res)
aggregate(USArrests, by=list(cluster=km.res$cluster), mean)
# View the firt 3 rows of the data
head(df, n = 3)
set.seed(123)
km.res <- kmeans(df, 4, nstart = 25)
print(km.res)
aggregate(USArrests, by=list(cluster=km.res$cluster), mean)
dd <- cbind(USArrests, cluster = km.res$cluster)
head(dd)
km.res$cluster
head(km.res$cluster, 4)
km.res$size
test <- user_item
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
km.res <- clusterKmeans
aggregate(USArrests, by=list(cluster=km.res$cluster), mean)
aggregate(user_item, by=list(cluster=km.res$cluster), mean)
dd <- cbind(user_item, cluster = km.res$cluster)
head(dd)
km.res$cluster
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
groups <- clusterKmeans$cluster
clusterKmeans$cluster
# kmeans
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
aggregate(user_item, by=list(cluster=clusterKmeans$cluster), mean)
dd <- cbind(user_item, cluster = clusterKmeans$cluster)
head(dd)
clusterKmeans$cluster
# kmeans
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
# hierarchical clustering
d <- dist(test, method="euclidean")
pfit <- hclust(d, method="ward.D")
plot(pfit, labels=test$i11)
plot(pfit, labels=test$i32)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
class(m)
View(tt)
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
str(tt)
tt[which(tt == 0, arr.ind = TRUE)] <- NA
View(tt)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
class(m)
View(tt)
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
str(tt)
class(tt)
clusterKmeans <- kmeans(tt, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
d <- dist(tt, method="euclidean")
pfit <- hclust(d, method="ward.D")
plot(pfit, labels=tt$i1)
# test
rownames(tt)
# test
class(rownames(tt))
tt$user <-c(rownames(tt))
plot(pfit, labels=tt$user)
# hierarchical clustering
test$user <- c(rownames(test))
d <- dist(test, method="euclidean")
pfit <- hclust(d, method="ward.D")
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
# hierarchical clustering
test$user <- c(rownames(test))
d <- dist(test, method="euclidean")
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
d <- dist(test, method="euclidean")
pfit <- hclust(d, method="ward.D")
test$user <- c(rownames(test))
plot(pfit, labels=test$user)
pfit$labels
pfit$order
pfit$method
pfit$call
pfit$dist.method
as.matrix(d)[1:4,]
plot(pfit, hand=-1, cex=0.8)
plot(pfit, hang=-1, cex=0.8)
plot(pfit, hang=3, cex=0.8)
plot(pfit, hang=-10, cex=0.8)
# kmeans
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
clusterKmeans <- kmeans(tt, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
clusterKmeans <- kmeans(tt, centers = 3, iter.max = 1000)
summary(clusterKmeans)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
tt$user <-c(rownames(tt))
d <- dist(tt, method="euclidean")
pfit <- hclust(d, method="ward.D")
plot(pfit, labels=tt$user)
# hierarchical clustering
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
tt$user <-c(rownames(tt))
d <- dist(tt, method="euclidean")
View(tt)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
d <- dist(tt, method="euclidean")
pfit <- hclust(d, method="ward.D")
tt$user <-c(rownames(tt))
plot(pfit, labels=tt$user)
# hierarchical clustering
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
d <- dist(test, method="euclidean")
as.matrix(d)[1:4,]
pfit <- hclust(d, method="ward.D")
test$user <- c(rownames(test))
#plot(pfit, hang=-10, cex=0.8)
plot(pfit, labels=test$user)
plot(pfit, hang=-1, cex=0.8)
synth.data <- data.frame(x1 =c(rnorm(20, 3, 1), rnorm(20,0,1), rnorm(20,5,1)),
x2 =c(rnorm(20, 0, 1), rnorm(20,4,1), rnorm(20,5,1)))
synth.data
library("RColorBrewer")
library("RColorBrewer")
ggplot(test) +
geom_hline(yintercept = 1:30, colour="grey") +
geom_vline(xintercept = 1:30, colour="grey") +
geom_point(aes(x=X1,y=X2,col=as.factor(clusterKmeans$cluster)), cex=5) +
scale_color_manual(values = c("black",brewer.pal(n = 9, name = 'Set1'))) +
theme_bw() + theme(legend.position = "None")
library(ggplot2)
library("ggplot2")
library("RColorBrewer")
ggplot(test) +
geom_hline(yintercept = 1:30, colour="grey") +
geom_vline(xintercept = 1:30, colour="grey") +
geom_point(aes(x=X1,y=X2,col=as.factor(clusterKmeans$cluster)), cex=5) +
scale_color_manual(values = c("black",brewer.pal(n = 9, name = 'Set1'))) +
theme_bw() + theme(legend.position = "None")
library(cluster)
library(compareGroups)
library(HDclassif)
library(NbClust)
library(sparcl)
data(wine)
str(wine)
install.packages("compareGroups")
installed.packages("compareGroups")
installed.packages("HDclassif")
library(cluster)
library(compareGroups)
library(HDclassif)
install.packages("HDclassif")
installed.packages("NbClust")
library(HDclassif)
library(NbClust)
install.packages("NbClust")
library(sparcl)
installed.packages("sparcl")
library(sparcl)
install.packages("sparcl")
library(sparcl)
data(wine)
str(wine)
View(wine)
df <- as.data.frame(scale(wine[,-1]))
numComplete <- NbClust(df, distance="euclidean", min.nc=2, max.nc=6, method="complete", index="all")
library(NbClust)
#numComplete <- NbClust(df, distance="euclidean", min.nc=2, max.nc=6, method="complete", index="all")
dis <- dist(df, method="euclidean")
hc <- hclust(dis, method="complete")
plot(hc, hang=-1, labels=FALSE, main="test")
plot(hc, labels=FALSE, main="test")
comp3 <- cutree(hc, 3)
ColorDendrogram(hc, y=comp3, main="test")
ColorDendrogram(hc, y=comp3, main="test")
hc <- hclust(dis, method="complete")
plot(hc, hang=-1, labels=FALSE, main="test")
comp3 <- cutree(hc, 3)
ColorDendrogram(hc, y=comp3, main="test")
comp3 <- cutree(hc, 3)
ColorDendrogram(hc, y=comp3, main="test")
hcWard <- hclust(dis, method="ward.D2")
plot(hcWard, labels=FALSE, main="Ward")
ward3 <- cutree(hcWard, 3)
table(ward3, wine$class)
colnames(wine)
numComplete <- NbClust(df, distance="euclidean", min.nc=2, max.nc=6, method="complete", index="all")
#
numKMeans <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
km <- kmeans(df, 3, nstart=25)
set.seed(1234)
km <- kmeans(df, 3, nstart=25)
table(km$cluster)
numKMeans <- NbClust(test, min.nc=2, max.nc=15, method="kmeans")
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
numKMeans <- NbClust(test, min.nc=2, max.nc=15, method="kmeans")
set.seed(1234)
km <- kmeans(df, 3, nstart=25)
table(km$cluster)
km <- kmeans(test, 3, nstart=25)
table(km$cluster)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
km <- kmeans(tt, 3, nstart=25)
table(km$cluster)
km$cluster
test <- t(user_item)
test[is.na(test)] <- 0
test <- as.data.frame(test)
clusterKmeans <- kmeans(test, centers = 3, iter.max = 1000)
summary(clusterKmeans)
clusterKmeans$cluster
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
clusterKmeans <- kmeans(tt, centers = 3, iter.max = 1000)
clusterKmeans$cluster
numKMeans <- NbClust(test, min.nc=2, max.nc=15, method="kmeans")
set.seed(1234)
km <- kmeans(tt, 3, nstart=25)
km$cluster
table(km$cluster)
df <- read.csv("pohang.csv")
m <- df[, -12]
m <- t(m)
m <- m[-1,]
colnames(m) <- paste("i", 1:ncol(m), sep='')
tt <- as.data.frame(m)
tt[,c(1:ncol(tt))] <- as.double(unlist(tt[,c(1:ncol(tt))]))
clusterKmeans <- kmeans(tt, centers = 3, iter.max = 1000)
clusterKmeans$cluster
set.seed(1234)
km <- kmeans(tt, 3, nstart=25)
km$cluster
table(km$cluster)
library(stringr)
library(dplyr)
src_dir <- c('/Users/osangjin/Desktop/past/Recommendation-system/data')
src_file <- list.files(src_dir) # list
src_file_cnt <- length(src_file)
src_file_cnt
id_lst <- c()
for(i in 1:src_file_cnt) {
file_name <- gsub(pattern=' ', replacement = '', paste(src_dir, '/', src_file[i]))
user_id <- as.numeric(str_extract(src_file[i], '\\d+'))
df <- read.csv(file_name)
df_name <- paste('df',user_id, sep='')
id_lst[i] <- user_id
df <- df[, -1]
assign(df_name, df)
}
list_df_data <- mget(ls(pattern = "^df\\d+"))
list_df_data
check_df <- function(df, i){
df_type <- class(df[[i]])
if (df_type[1] != "data.frame") {
df[[2]] <- as.data.frame(df[[2]])
df[[2]] <- t(df[[2]])
df[[2]][,2] <- as.numeric(df[[2]][,2])
}
return (df[[i]])
}
A <- list_df_data[[1]]
colnames(A) <- c("title", 1)
merge_df <- A
i = 2
repeat{
if(i>src_file_cnt-1) break
B <- check_df(list_df_data,i)
colnames(B) <- c("title", i)
merge_df <- merge(x=merge_df, y=B, by='title', all=TRUE)
i <- i+1
}
restaurant_lst <- as.vector(merge_df[,1])
restaurant_lst
user_item <- merge_df[,-1]
rownames(user_item) <- paste("i", 1:nrow(user_item), sep='')
colnames(user_item) <- paste("u", 1:ncol(user_item), sep='')
user_item <- user_item[which(rowSums(!is.na(user_item)) > 3),]
# View(user_item)
setwd("~/Desktop/past/Esemble")
# credit_train, temperature_train 데이터 활용
load("C:/Users/sec/Desktop/개인 업무 자료/DRA/R 캠프 앙상블/EnsembleforDay4.RData")
## Bagging and Boosting 개념 요약
### 1. Bagging 직접 구현 방법
str(spamD)
setwd("~/Desktop/past/Esemble")
# credit_train, temperature_train 데이터 활용
load("EnsembleforDay4.RData")
# credit_train, temperature_train 데이터 활용
load("edufoorday4.RData")
# credit_train, temperature_train 데이터 활용
load("eduforday4.RData")
## Bagging and Boosting 개념 요약
### 1. Bagging 직접 구현 방법
str(spamD)
# 이제 본격적으로 앙상블 구현을 해보자
dim(spamD)
ntrain <- dim(spamD)[1]
n <- ntrain
ntree <- 100
## 분할
library(caret)
spam_idx <- createDataPartition(spamD$spam, p=.8, list=F)
spam_tr <- spamD[spam_idx, ]
spam_te <- spamD[-spam_idx, ]
## 20000개의 랜덤한 샘플들을 5개 만들어보자 그리고 그 샘플들에 모델링을 적용해보자..!
library(rpart)
sample1 <- sample(1:ntrain, size=n, replace=T)
bag_model1 <- rpart(spam~., data=spam_tr[sample1,])
sample2 <- sample(1:ntrain, size=n, replace=T)
bag_model2 <- rpart(spam~., data=spam_tr[sample2,])
sample3 <- sample(1:ntrain, size=n, replace=T)
bag_model3 <- rpart(spam~., data=spam_tr[sample3,])
sample4 <- sample(1:ntrain, size=n, replace=T)
bag_model4 <- rpart(spam~., data=spam_tr[sample4,])
sample5 <- sample(1:ntrain, size=n, replace=T)
bag_model5 <- rpart(spam~., data=spam_tr[sample5, ])
pre1 <- predict(bag_model1, newdata=spam_te)[,2]
pre2 <- predict(bag_model2, newdata=spam_te)[,2]
pre3 <- predict(bag_model3, newdata=spam_te)[,2]
pre4 <- predict(bag_model4, newdata=spam_te)[,2]
pre5 <- predict(bag_model5, newdata=spam_te)[,2]
pred_sums <- (pre1+pre2+pre3+pre4+pre5)
pred_sums <- pred_sums/5
# predict.bag에서 산출된 확률을 threshold로 사용하는 것!
# 정확도 테스트 하는 부분
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(actual = truth, pred =ifelse(pred>mean(pred),1,0))
accuracy <- sum(diag(ctable))/sum(ctable)
precision <- ctable[2,2]/sum(ctable[,2])
recall <- ctable[2,2]/sum(ctable[2,])
f1 <- 2*precision*recall/(precision+recall)
data.frame(model=name, accuracy=accuracy,precision=precision, recall=recall, f1=f1)
}
accuracyMeasures(pre1, spam_te$spam)
accuracyMeasures(pred_sums, spam_te$spam)
# 그렇다면 이제 그 sample들 5개가 아닌 100개를 만들어 수행해보자
samples <- sapply(1:ntree, FUN = function(iter){sample(1:ntrain,size=n,replace=T)})
head(samples,3)
set.seed(123)
head(treelists,2)
predict.bag <- function(treelist, newdata){
preds <- sapply(1:length(treelist),
FUN=function(iter){
predict(treelist[[iter]],newdata=newdata)[,2]
})
predsums <- rowSums(preds)
predsums/length(treelist)
}
p <- sapply(1:length(treelists),FUN=function(iter){predict(treelists[[iter]],newdata=spam_te)[,2]})
psum <- rowSums(p)
credit_bg <- credit_train
View(credit_bg)
str(credit_bg)
# caret 패키지의 createDataPartition 함수를 통해 train, test데이터를 나눈다.
library(caret)
bg_indx <- createDataPartition(credit_train_bg$default.payment.next.month, p=.8, list=F)
credit_bg1 <- credit_bg[bg_indx, ]
credit_bg2 <- credit_bg[-bg_indx, ]
##### <xgboost>
library(xgboost)
str(credit_bg1)
library(dplyr)
# 데이터를 매트릭스 형태로 바꾸어줌
x_credit <- credit_bg1 %>% select(-default.payment.next.month) %>% data.matrix
y_credit <- credit_bg1$default.payment.next.month
#### < Regression >
temperature_train
temperature_test
str(temperature_train)
str(temperature_test)
temp_idx <- createDataPartition(temperature_train$Y18, p=.8, list=F)
temp_tr <- temperature_train[temp_idx, ]
temp_te <- temperature_train[-temp_idx, ]
str(temp_tr)
str(temp_te)
# id항목 아예 제거
temp_tr <- temp_tr[-1]
temp_te <- temp_te[-1]
set.seed(2020)
rf_temp <- randomForest(Y18~., data=temp_tr)
rf_temp$ntree
rf_temp$mtry
rf_temp <- randomForest(Y18~., data=temp_tr)
library(randomForest)
rf_temp <- randomForest(Y18~., data=temp_tr)
rf_temp$ntree
rf_temp$mtry
rf_temp$ntree
rf_temp$mtry
pred_temp1 <- predict(rf_temp, temp_tr)
pred_temp1
pred_temp2 <- predict(rf_temp, temp_te)
pred_temp2
calcRMSE <- function(label, estimation){
return(sqrt(mean((label-estimation)**2)))
}
calcRMSE(temp_tr$Y18, pred_temp1)
calcRMSE(temp_te$Y18, pred_temp2)
# ntree의 갯수를 조금 줄여서 실행해보자
set.seed(2020)
rf_temp2 <- randomForest(Y18~., data=temp_tr, ntree=300)
pred_temp3 <- predict(rf_temp2, temp_tr)
pred_temp3
pred_temp4 <- predict(rf_temp2, temp_te)
pred_temp4
calcRMSE(temp_tr$Y18, pred_temp3)
calcRMSE(temp_te$Y18, pred_temp4)
# 마찬가지로 hyper parameter tuning을 해보면 어떻게 될까...?
fitContrl2 <- trainControl(method='cv', number=3, search='grid')
grids2 <- expand.grid(.mtry=c(10:15))
modellists2 <- list()
for (ntrees in c(100,300,500,700)){
set.seed(2020)
rf_fit2 <- train(Y18~., data=temp_tr, method='rf', metric = 'RMSE', tuneGrid = grids2, trControl=fitContrl2,ntree=ntrees,verbose=T)
key2 <- toString(ntrees)
modellists2[[key2]] <- rf_fit2
}
modellists2
a <- read.table("come.RData")
a <- read.table("come.RData")
setwd("~/Desktop")
load("come.RData")
View(Jester5k)
class(Jester5k)
as(Jester5k, "matrix")
View(as(Jester5k, "matrix"))
library(recommenderlab)
data(Jester)
data("Jester5k")
View(Jester5k)
View(Jester5k)
setwd("~/Desktop/past/Recommendation-system/source/recommend")
library(stringr)
library(dplyr)
src_dir <- c('/Users/osangjin/Desktop/past/Recommendation-system/data')
src_file <- list.files(src_dir)
src_file_cnt <- length(src_file)
src_file_cnt
paste(src_dir, '/', src_file[1])
gsub(pattern=' ', replacement = '', paste(src_dir, '/', src_file[1]))
